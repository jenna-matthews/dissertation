{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55c1cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the key libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "#need to store the data in a dataframe\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061fdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the variables\n",
    "debugFlag = 0\n",
    "uniqueid = 0\n",
    "\n",
    "gowers_article_list = []\n",
    "tao_article_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf7ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [post_source, post_url, post_timestamp, comment_timestamp, post_title, counter, post_author, post_word_count, post_text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#create the empty dataframe for initial posts and comments\n",
    "polymath_BERT = pd.DataFrame(columns = ['post_source','post_url','post_timestamp', 'comment_timestamp','post_title','counter', 'post_author', 'post_word_count','post_text'])\n",
    "print(polymath_BERT)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d737300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reads from csv source and appends to article link list\n",
    "#changes from source code:\n",
    "# switched to 'r' so it wasn't a binary file; \n",
    "# added encoding - the utf-8-sig removes the '\\ufeff' that was showing up at the beginning of the first row\n",
    "with open('/Users/jennaolsen/Desktop/mothership-gowers-articles-list.csv', 'r', encoding=\"unicode_escape\") as f:\n",
    "\treader = csv.reader(f)\n",
    "\tfor row in reader:\n",
    "\t\tgowers_article_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6980209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reads from csv source and appends to article link list\n",
    "#changes from source code:\n",
    "# switched to 'r' so it wasn't a binary file; \n",
    "# added encoding - the utf-8-sig removes the '\\ufeff' that was showing up at the beginning of the first row\n",
    "with open('/Users/jennaolsen/Desktop/mothership-tao-articles-list.csv', 'r', encoding=\"unicode_escape\") as f:\n",
    "\treader = csv.reader(f)\n",
    "\tfor row in reader:\n",
    "\t\ttao_article_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###  Checks the content of the article_link_list\n",
    "# for row in gowers_article_list:\n",
    "# \tprint((row))\n",
    "# print(len(gowers_article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f85278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now scraping: book\u0000\u0000\u0000\u0000mark\u0000\u0000\u0000\u00008\u0000\u0000\u00008\u0000\u0000\u0000|\u0003\u0000\u0000\u0000\u0000\u0004\u0010\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000tî`\u0006îÅA\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000T\u0002\u0000\u0000\u0004\u0000\u0000\u0000\u0003\u0003\u0000\u0000\u0000\u0004\u0000\u0000\u0005\u0000\u0000\u0000\u0001\u0001\u0000\u0000Users\u0000\u0000\u0000...\n"
     ]
    },
    {
     "ename": "InvalidURL",
     "evalue": "Failed to parse: book\u0000\u0000\u0000\u0000mark\u0000\u0000\u0000\u00008\u0000\u0000\u00008\u0000\u0000\u0000|\u0003\u0000\u0000\u0000\u0000\u0004\u0010\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000tî`\u0006îÅA\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000T\u0002\u0000\u0000\u0004\u0000\u0000\u0000\u0003\u0003\u0000\u0000\u0000\u0004\u0000\u0000\u0005\u0000\u0000\u0000\u0001\u0001\u0000\u0000Users\u0000\u0000\u0000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLocationParseError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/urllib3/util/url.py:440\u001b[0m, in \u001b[0;36mparse_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    438\u001b[0m     port_int \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m host \u001b[38;5;241m=\u001b[39m _normalize_host(host, scheme)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize_uri \u001b[38;5;129;01mand\u001b[39;00m path:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/urllib3/util/url.py:326\u001b[0m, in \u001b[0;36m_normalize_host\u001b[0;34m(host, scheme)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _IPV4_RE\u001b[38;5;241m.\u001b[39mmatch(host):\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m to_str(\n\u001b[0;32m--> 326\u001b[0m                 \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([_idna_encode(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m host\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)]),\n\u001b[1;32m    327\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    328\u001b[0m             )\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m host\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/urllib3/util/url.py:326\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _IPV4_RE\u001b[38;5;241m.\u001b[39mmatch(host):\n\u001b[1;32m    325\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m to_str(\n\u001b[0;32m--> 326\u001b[0m                 \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([_idna_encode(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m host\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)]),\n\u001b[1;32m    327\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    328\u001b[0m             )\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m host\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/urllib3/util/url.py:344\u001b[0m, in \u001b[0;36m_idna_encode\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m idna\u001b[38;5;241m.\u001b[39mIDNAError:\n\u001b[0;32m--> 344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\n\u001b[1;32m    345\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid IDNA label\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m name\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLocationParseError\u001b[0m: Failed to parse: Name 'book\u0000\u0000\u0000\u0000mark\u0000\u0000\u0000\u00008\u0000\u0000\u00008\u0000\u0000\u0000|\u0003\u0000\u0000\u0000\u0000\u0004\u0010\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000tî`\u0006îÅA\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000T\u0002\u0000\u0000\u0004\u0000\u0000\u0000\u0003\u0003\u0000\u0000\u0000\u0004\u0000\u0000\u0005\u0000\u0000\u0000\u0001\u0001\u0000\u0000Users\u0000\u0000\u0000' is not a valid IDNA label",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocationParseError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/models.py:434\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m     scheme, auth, host, port, path, query, fragment \u001b[38;5;241m=\u001b[39m parse_url(url)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LocationParseError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/urllib3/util/url.py:451\u001b[0m, in \u001b[0;36mparse_url\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(source_url) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# For the sake of backwards compatibility we put empty\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# string values for path if there are any defined values\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# beyond the path in the URL.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# TODO: Remove this when we break backwards compatibility.\u001b[39;00m\n",
      "\u001b[0;31mLocationParseError\u001b[0m: Failed to parse: book\u0000\u0000\u0000\u0000mark\u0000\u0000\u0000\u00008\u0000\u0000\u00008\u0000\u0000\u0000|\u0003\u0000\u0000\u0000\u0000\u0004\u0010\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000tî`\u0006îÅA\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000T\u0002\u0000\u0000\u0004\u0000\u0000\u0000\u0003\u0003\u0000\u0000\u0000\u0004\u0000\u0000\u0005\u0000\u0000\u0000\u0001\u0001\u0000\u0000Users\u0000\u0000\u0000",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m article_link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(article_link)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNow scraping: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m article_link \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(article_link)\n\u001b[1;32m     21\u001b[0m soupData \u001b[38;5;241m=\u001b[39m BeautifulSoup(r\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m### Scraping the title - had to remove the encoding because it killed concate operation in the next line\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Create the Request.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[1;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(req)\n\u001b[1;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    579\u001b[0m settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_environment_settings(\n\u001b[1;32m    580\u001b[0m     prep\u001b[38;5;241m.\u001b[39murl, proxies, stream, verify, cert\n\u001b[1;32m    581\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/sessions.py:486\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    483\u001b[0m     auth \u001b[38;5;241m=\u001b[39m get_netrc_auth(request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    485\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[0;32m--> 486\u001b[0m p\u001b[38;5;241m.\u001b[39mprepare(\n\u001b[1;32m    487\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m    488\u001b[0m     url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl,\n\u001b[1;32m    489\u001b[0m     files\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mfiles,\n\u001b[1;32m    490\u001b[0m     data\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m    491\u001b[0m     json\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mjson,\n\u001b[1;32m    492\u001b[0m     headers\u001b[38;5;241m=\u001b[39mmerge_setting(\n\u001b[1;32m    493\u001b[0m         request\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, dict_class\u001b[38;5;241m=\u001b[39mCaseInsensitiveDict\n\u001b[1;32m    494\u001b[0m     ),\n\u001b[1;32m    495\u001b[0m     params\u001b[38;5;241m=\u001b[39mmerge_setting(request\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams),\n\u001b[1;32m    496\u001b[0m     auth\u001b[38;5;241m=\u001b[39mmerge_setting(auth, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauth),\n\u001b[1;32m    497\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mmerged_cookies,\n\u001b[1;32m    498\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mmerge_hooks(request\u001b[38;5;241m.\u001b[39mhooks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks),\n\u001b[1;32m    499\u001b[0m )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/models.py:368\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prepares the entire request with the given parameters.\"\"\"\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_url(url, params)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_cookies(cookies)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/models.py:436\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    434\u001b[0m     scheme, auth, host, port, path, query, fragment \u001b[38;5;241m=\u001b[39m parse_url(url)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LocationParseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;241m*\u001b[39me\u001b[38;5;241m.\u001b[39margs)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m     )\n",
      "\u001b[0;31mInvalidURL\u001b[0m: Failed to parse: book\u0000\u0000\u0000\u0000mark\u0000\u0000\u0000\u00008\u0000\u0000\u00008\u0000\u0000\u0000|\u0003\u0000\u0000\u0000\u0000\u0004\u0010\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000tî`\u0006îÅA\u0000\u0000\u0000\u0000\u0001\u0000\u0000\u0000T\u0002\u0000\u0000\u0004\u0000\u0000\u0000\u0003\u0003\u0000\u0000\u0000\u0004\u0000\u0000\u0005\u0000\u0000\u0000\u0001\u0001\u0000\u0000Users\u0000\u0000\u0000"
     ]
    }
   ],
   "source": [
    "# pulling data from the gowers primary posts\n",
    "for article_link in gowers_article_list:\n",
    "\n",
    "\t# Masking nature of the bot\n",
    "\tif int(datetime.datetime.now().second) % 3 == 0:\n",
    "\t\tuser_agent = 'Mozilla/5 (Solaris 10) Gecko'\n",
    "\telif int(datetime.datetime.now().second) % 5 == 0:\n",
    "\t\tuser_agent = 'Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)'\n",
    "\telse:\n",
    "\t\tuser_agent = 'Mozilla/5.0 (compatible; MSIE 5.5; Windows NT)'\n",
    "\n",
    "\theaders = {'User-Agent': user_agent}\n",
    "\n",
    "\ttime.sleep(1.4)\n",
    "\n",
    "# \tprint \"\\n\\n=====Task ID: \" + str(uniqueid) + \"=====\\n\"\n",
    "\n",
    "\tarticle_link = ''.join(article_link)\n",
    "\tprint (\"Now scraping: \" + article_link + \"...\")\n",
    "\tr = requests.get(article_link)\n",
    "\tsoupData = BeautifulSoup(r.text)\n",
    "\n",
    "\t### Scraping the title - had to remove the encoding because it killed concate operation in the next line\n",
    "\ttitle = soupData.find('h2').text\n",
    "# \ttitle = title.encode('utf-8-sig').strip()\n",
    "# \tprint (\"Title: \" + title)\n",
    "\n",
    "# <meta property=\"article:published_time\" content=\"2009-02-01T12:43:20+00:00\" />\n",
    "\n",
    "\t### Scraping the timestamp\n",
    "\ttimestamp = soupData.find(\"meta\", property=\"article:published_time\")\n",
    "\tts = timestamp[\"content\"]\n",
    "\tprint (\"Published Time: \" + ts.replace('T',' ').replace('+00:00',''))\n",
    "\n",
    "\t### Scraping the entry\n",
    "\tentry = soupData.find_all(\"div\", {\"class\": \"entry\"})\n",
    "\tpost_content = str(entry)\n",
    "\t### Parsing the text into a beautfulsoup item\n",
    "\tparsed_post_content = BeautifulSoup(post_content)\n",
    "\t### Find all paragraphs/blog content\n",
    "\tcontent_tokens = parsed_post_content.findAll('p')\n",
    "\t### The last 2 paragraphs are unwanted\n",
    "# \tadvert_text = content_tokens[len(content_tokens)-1]\n",
    "\t### parse to string form\n",
    "\tadvert_text = str(content_tokens)\n",
    "\t### parse to string form as well\n",
    "# \tpost_content = parsed_post_content.text\n",
    "\tpost_content = post_content.replace(advert_text, \"\")\n",
    "\t### To remove all newlines\n",
    "\tpost_content = post_content.replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()\n",
    "\t### To remove all tags\n",
    "\tpost_content = BeautifulSoup(post_content, \"lxml\").text\n",
    "    ### To remove the garbage left at the end of the text\n",
    "\tpost_content = post_content.replace(\"Share this:TwitterFacebookLike this:Like Loading...Related\", \"\").strip()#.encode('utf-8')\n",
    "    \n",
    "    ###word count for the post \n",
    "\tword_count = len(post_content.split())\n",
    "    \n",
    "    ###Add the new row to the dataframe \n",
    "\tnew_row = {'post_source':'Gowers','post_url':article_link,'post_timestamp':ts,'post_title':title,'counter':'0', 'post_author':'gowers', 'post_word_count':word_count, 'post_text':post_content}\n",
    "\tpolymath_BERT = polymath_BERT.append(new_row, ignore_index=True)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling comments from the various articles\n",
    "# move into main section once it is working\n",
    "for article_link in gowers_article_list:\n",
    "\n",
    "\t# Masking nature of the bot\n",
    "\tif int(datetime.datetime.now().second) % 3 == 0:\n",
    "\t\tuser_agent = 'Mozilla/5 (Solaris 10) Gecko'\n",
    "\telif int(datetime.datetime.now().second) % 5 == 0:\n",
    "\t\tuser_agent = 'Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)'\n",
    "\telse:\n",
    "\t\tuser_agent = 'Mozilla/5.0 (compatible; MSIE 5.5; Windows NT)'\n",
    "\n",
    "\theaders = {'User-Agent': user_agent}\n",
    "\n",
    "\ttime.sleep(1.4)\n",
    "\n",
    "\tarticle_link = ''.join(article_link)\n",
    "\tprint (\"Now scraping: \" + article_link + \"...\") # the \"post_url\" field\n",
    "\tr = requests.get(article_link)\n",
    "\tsoupData = BeautifulSoup(r.text)\n",
    "    \n",
    "### Scraping for comments\n",
    "\tcomments_count = soupData.find_all(\"li\", attrs={\"id\" : True})\n",
    "\tfor cc in comments_count:\n",
    "\t\tident = cc[\"id\"].replace(\"comment-\",\"C\").strip()\n",
    "# \t\tprint(ident) #this will be the \"counter\" field\n",
    "        \n",
    "\t\ttime_string = cc.find(\"small\").text.replace(\" | Reply \",\"\").strip()\n",
    "# \t\tprint(time_string) # the \"post_timestamp\" field\n",
    "        \n",
    "\t\tthe_comment = str(cc.findAll(\"p\"))\n",
    "\t\tthe_comment = BeautifulSoup(the_comment, \"lxml\").text\n",
    "# \t\tprint(the_comment) # the \"post_text\" field\n",
    "\n",
    "\t\tauthor = cc.find(\"cite\").string\n",
    "# \t\tprint(author) # the \"post_text\" field\n",
    "\n",
    "\t\tword_count = len(str(the_comment).split())\n",
    "\n",
    "        ###insert the rw into the dataframe\n",
    "\t\tnew_row = {'post_source':'Gowers','post_url':article_link,'comment_timestamp':time_string,'post_title':'','counter':ident, 'post_author':author, 'post_word_count':word_count, 'post_text':the_comment}\n",
    "\t\tpolymath_BERT = polymath_BERT.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f404392",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Checks the content of the article_link_list\n",
    "for row in tao_article_list:\n",
    "\tprint((row))\n",
    "print(len(tao_article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86978518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling data from the various articles\n",
    "for article_link in tao_article_list:\n",
    "\n",
    "\t# Masking nature of the bot\n",
    "\tif int(datetime.datetime.now().second) % 3 == 0:\n",
    "\t\tuser_agent = 'Mozilla/5 (Solaris 10) Gecko'\n",
    "\telif int(datetime.datetime.now().second) % 5 == 0:\n",
    "\t\tuser_agent = 'Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)'\n",
    "\telse:\n",
    "\t\tuser_agent = 'Mozilla/5.0 (compatible; MSIE 5.5; Windows NT)'\n",
    "\n",
    "\theaders = {'User-Agent': user_agent}\n",
    "\n",
    "\ttime.sleep(1.4)\n",
    "\n",
    "\tarticle_link = ''.join(article_link)\n",
    "\tprint (\"Now scraping: \" + article_link + \"...\")\n",
    "\tr = requests.get(article_link)\n",
    "\tsoupData = BeautifulSoup(r.text)\n",
    "\n",
    "\t### Scraping the title - had to remove the encoding because it killed concate operation in the next line\n",
    "\ttitle = soupData.find('title').text.replace(\" | What's new\",\"\")\n",
    "# \tprint (\"Title: \" + title)\n",
    "\n",
    "\t### Scraping the timestamp\n",
    "\ttimestamp = soupData.find(\"meta\", property=\"article:published_time\")\n",
    "# \tprint (\"Published Time: \" + timestamp[\"content\"])\n",
    "\tts = timestamp[\"content\"]\n",
    "\n",
    "\t### Scraping the entry\n",
    "\tentry = soupData.find_all(\"div\", {\"class\": \"post-content\"})\n",
    "\tpost_content = str(entry)\n",
    "\t### Parsing the text into a beautfulsoup item\n",
    "\tparsed_post_content = BeautifulSoup(post_content)\n",
    "\t### parse to string form\n",
    "\tadvert_text = parsed_post_content.text\n",
    "\t## parse to string form as well\n",
    "\tpost_content = advert_text\n",
    "    ### To remove the garbage left at the end of the text\n",
    "\tpost_content = post_content.replace(\"Further articles on this project are collected at this page.\", \"\").replace(\"<\\b>\", \"\")\n",
    "\tpost_content = post_content.replace(\"\\n\\nShare this:PrintEmailMoreTwitterFacebookRedditPinterestLike this:Like Loading... \", \"\").strip().encode('utf-8')\n",
    "# \tprint (post_content)\n",
    "   \n",
    "\t## parse to string form as well\n",
    "\tpost_content = BeautifulSoup(post_content, \"lxml\").text\n",
    "   \n",
    "\t##word count of post\n",
    "\tword_count = len(post_content.split())\n",
    "    \n",
    "#     ###Add the new row to the dataframe \n",
    "\tnew_row = {'post_source':'Tao','post_url':article_link,'post_timestamp':ts,'post_title':title,'counter':'0', 'post_author':'Tao', 'post_word_count':word_count, 'post_text':post_content}\n",
    "\tpolymath_BERT = polymath_BERT.append(new_row, ignore_index=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb61773",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article_link in tao_article_list:\n",
    "\n",
    "\t# Masking nature of the bot\n",
    "\tif int(datetime.datetime.now().second) % 3 == 0:\n",
    "\t\tuser_agent = 'Mozilla/5 (Solaris 10) Gecko'\n",
    "\telif int(datetime.datetime.now().second) % 5 == 0:\n",
    "\t\tuser_agent = 'Mozilla/4.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/5.0)'\n",
    "\telse:\n",
    "\t\tuser_agent = 'Mozilla/5.0 (compatible; MSIE 5.5; Windows NT)'\n",
    "\n",
    "\theaders = {'User-Agent': user_agent}\n",
    "\n",
    "\ttime.sleep(1.4)\n",
    "\n",
    "\tcomment_count = 0\n",
    "    \n",
    "\tarticle_link = ''.join(article_link)\n",
    "\tprint (\"Now scraping: \" + article_link + \"...\") # the \"post_url\" field\n",
    "\tr = requests.get(article_link)\n",
    "\tsoupData = BeautifulSoup(r.text)\n",
    "    \n",
    "### Scraping for comments\n",
    "### Comments for Tao show up with two different tags (so far) - this structure allows find_all to find both of them  'div[class*=\"listing-col-\"]'\n",
    "\tcomments_count = soupData.select('div[class*=\"-comment\"]')\n",
    "\tfor cc in comments_count:\n",
    "        #Adding the CTao in front of the number to avoid duplication with Gowers blog posts\n",
    "\t\tident = cc[\"id\"].replace(\"comment-\",\"CTao_\").strip()\n",
    "# \t\tprint(ident) #this will be the \"counter\" field\n",
    "\n",
    "\n",
    "        #Temporary to identify the posts where I am missing some comments\n",
    "\t\tcomment_count+=1\n",
    "# \t\tprint(ident) #this will be the \"counter\" field\n",
    "\n",
    "        \n",
    "\t\ttime_string = cc.find(\"p\", attrs={\"class\":\"comment-permalink\"}).text.strip()\n",
    "# \t\tprint(time_string) # the \"post_timestamp\" field\n",
    "        \n",
    "\t\tthe_comment = str(cc.find(\"div\", attrs={\"class\":\"comment-content\"}).findAll(\"p\"))\n",
    "\t\tthe_comment = BeautifulSoup(the_comment, \"lxml\").text\n",
    "# \t\tprint(the_comment) # the \"post_text\" field\n",
    "        \n",
    "\t\tthe_author = cc.find(\"p\", attrs={\"class\":\"comment-author\"}).find(\"strong\").string\n",
    "# \t\tprint(the_author) # the \"post_text\" field\n",
    "        \n",
    "\t\tword_count = len(str(the_comment).split())\n",
    "\n",
    "        ###insert the rw into the dataframe\n",
    "\t\tnew_row = {'post_source':'Tao','post_url':article_link,'comment_timestamp':time_string,'post_title':'','counter':ident, 'post_author':the_author, 'post_word_count':word_count, 'post_text':the_comment}\n",
    "\t\tpolymath_BERT = polymath_BERT.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d162a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ready to set up for the BERT analysis now :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2344f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking for the full count\n",
    "### 1543 comments and 20 posts from Gowers\n",
    "### 888 comments and 14 posts from Tao\n",
    "print(polymath_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polymath_BERT.to_excel('polymath_BERT_wordCounts.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
